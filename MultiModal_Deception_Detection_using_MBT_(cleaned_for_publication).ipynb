{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AryamanJaggi/Deception-Detection-with-Transformers/blob/main/MultiModal_Deception_Detection_using_MBT_(cleaned_for_publication).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M84m9JupKg9J",
        "outputId": "87b7e44d-f45e-4f84-9c55-563bf8d2ff6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41535024128"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.memory_allocated(0)\n",
        "\n",
        "torch.cuda.get_device_properties(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "XUe6DyFpOvCx",
        "outputId": "bca8fe5e-245c-4b54-e752-3b10fea6e2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([510, 1408])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d74d7278eb39>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpermuted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#switches embedding and time dimension aka T,F to F,T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnormalizedVideoFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermuted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#because fps varies grossly across the dataset (10,25,29.97,30 fps) I try to average the features so they are close to 10 fps before I padd them to match the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0munusedvideo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Deceptive/trial_lie_001.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fps' is not defined"
          ]
        }
      ],
      "source": [
        "video = torch.load('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/DeceptiveFERTensors/trial_lie_001.pt', map_location=torch.device('cpu'))\n",
        "print(video.shape)\n",
        "\n",
        "permuted = torch.permute(video, (1,0))#switches embedding and time dimension aka T,F to F,T\n",
        "normalizedVideoFeatures = torch.nn.functional.avg_pool1d(permuted, math.ceil(fps/10))#because fps varies grossly across the dataset (10,25,29.97,30 fps) I try to average the features so they are close to 10 fps before I padd them to match the audio\n",
        "\n",
        "unusedvideo,audio,metadata = read_video('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Deceptive/trial_lie_001.mp4')\n",
        "fps = metadata['video_fps']\n",
        "audioSampleRate = metadata['audio_fps']\n",
        "data = cv2.VideoCapture('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Deceptive/trial_lie_001.mp4')\n",
        "frames = data.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "print('frames: ' + str(frames))\n",
        "seconds = frames/float(fps)\n",
        "print(\"seconds:\" + str(seconds))\n",
        "\n",
        "print(fps)\n",
        "print(audioSampleRate)\n",
        "\n",
        "print(audio.shape)\n",
        "audio = torchaudio.functional.resample(audio, audioSampleRate, 16000)\n",
        "print(audio.shape)\n",
        "audio = torch.mean(audio, dim=0)#takes mean and gets rid of channels dimesnion\n",
        "print(audio.shape)\n",
        "audio = audio.unsqueeze(0)\n",
        "video = video.unsqueeze(0)\n",
        "\n",
        "\n",
        "mainModel(video,audio)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww1PNBWoIKLL"
      },
      "outputs": [],
      "source": [
        "#device = 'cuda'\n",
        "#model_name='enet_b2_8'\n",
        "#fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "#mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)\n",
        "#test_transforms = transforms.Compose(\n",
        "#                [\n",
        "#                    transforms.Resize((fer.img_size,fer.img_size)),\n",
        "#                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                                std=[0.229, 0.224, 0.225])\n",
        "#                ]\n",
        "#            )\n",
        "\n",
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful'): # deceptive\n",
        "  name = os.path.join('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/TruthfulFERTensors',filename.replace('.mp4','.pt'))\n",
        "  print(name)\n",
        "  if(os.path.exists(name)):\n",
        "      print(\"already copied previously\")\n",
        "      continue\n",
        "\n",
        "  video,audio,metadata = (read_video(os.path.join('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful',filename)))#video in shape T,H,W,C\n",
        "  del audio, metadata\n",
        "  gc.collect()\n",
        "  T,H,W,C = video.shape\n",
        "  #print(\"video.shape: \")\n",
        "  #print(video.shape)\n",
        "  #print(\"video name: \" + filename)\n",
        "  #print(\"=\"*20)\n",
        "  #FERFeatures = torch.empty(T, 1408)\n",
        "  split_size = math.ceil(T/100.0)\n",
        "  listOfTensors = video.split(split_size)\n",
        "  del video\n",
        "  gc.collect()\n",
        "  for i in range(len(listOfTensors)):\n",
        "    TensorBatch = torch.squeeze(torch.from_numpy(videoEmbeddingExtraction(torch.unsqueeze(listOfTensors[i],0))))\n",
        "\n",
        "    if (i == 0):\n",
        "      FERFeatures = TensorBatch\n",
        "      continue\n",
        "    if (len(TensorBatch.shape) < 2):\n",
        "      TensorBatch = torch.unsqueeze(TensorBatch,0)\n",
        "    #print(listOfTensors[i])\n",
        "    #print(TensorBatch.shape)\n",
        "    #print(FERFeatures.shape)\n",
        "    FERFeatures = torch.cat((FERFeatures, TensorBatch))\n",
        "    del TensorBatch\n",
        "    gc.collect()\n",
        "\n",
        "  torch.save(FERFeatures, name)\n",
        "  torch.cuda.empty_cache()\n",
        "  del FERFeatures\n",
        "  gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6fJVEtqtsWP",
        "outputId": "8c2b593a-3923-417a-d8ad-a632f7c56353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.hsemotion/enet_b2_8.pt Compose(\n",
            "    Resize(size=(260, 260), interpolation=bilinear, max_size=None, antialias=warn)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda'\n",
        "model_name='enet_b2_8'\n",
        "fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)\n",
        "test_transforms = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize((fer.img_size,fer.img_size)),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                std=[0.229, 0.224, 0.225])\n",
        "                ]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t69A1KqsI4Fg"
      },
      "outputs": [],
      "source": [
        "#debugging Code to see all tensors stored in\n",
        "#res = []\n",
        "#for obj in gc.get_objects():\n",
        "#  if torch.is_tensor(obj):\n",
        "#    print(obj.size())\n",
        "#    res.append(np.prod(obj.size()))\n",
        "  # elif (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
        "  #   res.append(np.prod(obj.data.size()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t_QusMlr11_"
      },
      "outputs": [],
      "source": [
        "video,audio,metadata = (read_video('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Deceptive/trial_lie_001.mp4'))#video in shape T,H,W,C\n",
        "del audio, metadata\n",
        "gc.collect()\n",
        "T,H,W,C = video.shape\n",
        "#FERFeatures = torch.empty(T, 1408)\n",
        "split_size = math.ceil(T/1000.0)\n",
        "listOfTensors = video.split(1)\n",
        "del video\n",
        "gc.collect()\n",
        "FERFeatures = []\n",
        "for i in range(len(listOfTensors)):\n",
        "  print('='*20)\n",
        "  TensorBatch = videoEmbeddingExtraction(torch.unsqueeze(listOfTensors[i],0))\n",
        "  print(torch.cuda.memory_allocated(0)/1e9)\n",
        "  FERFeatures.append(TensorBatch)\n",
        "\n",
        "\n",
        "# name = '/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/DeceptiveFERTensors/trial_lie_001.pt'\n",
        "# torch.save(FERFeatures, name)\n",
        "# torch.cuda.empty_cache()\n",
        "# del FERFeatures\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lF9fHmK7g_a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "def videoEmbeddingExtraction (video): # manually implementing some of the stuff from HSME\n",
        "\n",
        "    #video dimensions are B T H W C\n",
        "    # dimensions for each frame (which extract face requires) B'(B*T) H W C\n",
        "    B,T,H,W,C = video.shape\n",
        "    #cv2_imshow(torch.squeeze(torch.squeeze(video[0,0,:,:,:])).cpu().numpy())\n",
        "    #mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)\n",
        "    def extract_face(frame):\n",
        "        bounding_boxes, probs = mtcnn.detect(frame, landmarks=False)\n",
        "        #print(\"frame.shape\")\n",
        "        #print(frame.shape)\n",
        "        #print(\"bounding_boxes.shape:\")\n",
        "        #print(bounding_boxes.shape)\n",
        "        #$$$$$$4print('1:',torch.cuda.memory_allocated(0)/1e9)\n",
        "        #print(bounding_boxes.shape)\n",
        "        #bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "        #bounding_box= torch.take_along_dim(torch.from_numpy(bounding_boxes), torch.from_numpy(np.argmax(probs,1)),1)\n",
        "        bounding_boxes2 = []\n",
        "        #bounding_boxes = np.vstack(bounding_boxes[:]).astype(np.float64)\n",
        "        #print(bounding_boxes.shape)\n",
        "        #bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "        for i in range (bounding_boxes.shape[0]):\n",
        "          indexOfBestFace = np.argmax(probs[i])\n",
        "          arrayOfBoundingBoxesForOneFrame = bounding_boxes[i]\n",
        "          #print(i)\n",
        "          if arrayOfBoundingBoxesForOneFrame is None:\n",
        "            print(name)\n",
        "            bounding_boxes2.append(np.array([1000000,0,0,0]))\n",
        "            continue\n",
        "          #print(\"\\narrayOfBoundingBoxesForOneFrame shape:\")\n",
        "          #print(arrayOfBoundingBoxesForOneFrame.shape)\n",
        "          #print(\"specific bounding box type:\" + arrayOfBoundingBoxesForOneFrame.type)\n",
        "          bounding_boxes2.append(arrayOfBoundingBoxesForOneFrame[indexOfBestFace])#originalbounding_boxes2.append(bounding_boxes[i, indexOfBestFact,:])\n",
        "\n",
        "        del indexOfBestFace, arrayOfBoundingBoxesForOneFrame, bounding_boxes\n",
        "        gc.collect()\n",
        "        #$$$$$$$print('2:',torch.cuda.memory_allocated(0)/1e9)\n",
        "        bounding_boxes2 = np.stack(bounding_boxes2)\n",
        "        bounding_boxes2 = torch.from_numpy(bounding_boxes2.astype('int32'))#.int()\n",
        "        #box = bounding_box.int()\n",
        "        batchlist = []\n",
        "        i = 0\n",
        "        height = 0\n",
        "        width = 0\n",
        "        for batch in range(bounding_boxes2.shape[0]):\n",
        "          i += 1\n",
        "          x1,y1,x2,y2=bounding_boxes2[batch,0:4]\n",
        "          if (x1 == 1000000 or x1 < 0 or x2 < 0 or y1 < 0 or y2 < 0):\n",
        "            batchlist.append(torch.zeros((100,100,3)))\n",
        "            i -= 1\n",
        "            continue\n",
        "\n",
        "          face_img=frame[batch, y1:y2,x1:x2,:] # batch, h, w, c\n",
        "          #print(face_img.shape)\n",
        "          batchlist.append(face_img)\n",
        "          height += face_img.shape[0]\n",
        "          width += face_img.shape[1]\n",
        "\n",
        "        if (i==0): # edge case for trial_lie_005\n",
        "          return torch.zeros(1, bounding_boxes2.shape[0], 1408)\n",
        "\n",
        "        averageHeigth = int(height/i)\n",
        "        averageWidth = int(width/i)\n",
        "        #$$$$$$$print('3:',torch.cuda.memory_allocated(0)/1e9)\n",
        "        #del bounding_boxes2\n",
        "        #gc.collect()\n",
        "\n",
        "        resize = transforms.Compose([transforms.Resize((averageHeigth,averageWidth),interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)])\n",
        "\n",
        "        for j in range(bounding_boxes2.shape[0]):#i should be equal to bounding_boxes2.shape[0]\n",
        "          if(torch.max(batchlist[j]).numpy() == 0):\n",
        "            print(\"hit\")\n",
        "            batchlist[j] = torch.zeros(3,averageHeigth, averageWidth)\n",
        "            continue\n",
        "          batchlist[j] = resize(torch.permute(batchlist[j], (2,0,1)))#swithcing from hwc to chw and then resizing image\n",
        "        #print('4:',torch.cuda.memory_allocated(0)/1e9)\n",
        "        del bounding_boxes2\n",
        "        gc.collect()\n",
        "        return torch.stack(batchlist)\n",
        "\n",
        "    facialRecognitionBatch  = video.reshape(B*T, H, W, C) #got rid of time dimension from input\n",
        "    del video\n",
        "    gc.collect()\n",
        "    #print(facialRecognitionBatch.shape)\n",
        "    #permuted = torch.permute(facialRecognitionBatch, (0, 2,3,1))# orignally in C H W for pytorch, after permute in H W C for openCV\n",
        "    #cv2_imshow(torch.squeeze(facialRecognitionBatch[0,:,:,:]).cpu().numpy())\n",
        "\n",
        "    #facialRecognitionBatchNumpy = facialRecognitionBatch.cpu().numpy()\n",
        "    #facialRecognitionBatchNumpyBGR = facialRecognitionBatchNumpy[:,:,:,::-1]\n",
        "    #tensorBatch = torch.from_numpy(facialRecognitionBatchNumpyBGR)\n",
        "    #red, green, blue = torch.split(facialRecognitionBatch,3,3)#converting to BGR for open cv\n",
        "    #facialRecognitionBatch_BGR = torch.cat(3, [blue,green,red])\n",
        "    #cv2_imshow(torch.squeeze(facialRecognitionBatch_BGR[0,:,:,:]).cpu().numpy())\n",
        "    face = extract_face(facialRecognitionBatch) # B C H W\n",
        "\n",
        "    if (len(face.shape) == 3): #for edge case in trial_lie_005\n",
        "      return face.cpu().numpy()\n",
        "\n",
        "    del facialRecognitionBatch\n",
        "    gc.collect()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    #model_name='enet_b2_8'\n",
        "    #fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "    #features=fer.extract_features(face)\n",
        "    #print(features)\n",
        "\n",
        "    #test_transforms = transforms.Compose(\n",
        "    #            [\n",
        "    #                transforms.Resize((fer.img_size,fer.img_size)),\n",
        "    #                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "    #                                            std=[0.229, 0.224, 0.225])\n",
        "    #            ]\n",
        "    #        )\n",
        "\n",
        "    #face = torch.permute(face, (0,3,1,2))\n",
        "    img_tensor = test_transforms(face.double())\n",
        "    #print('5:',torch.cuda.memory_allocated(0)/1e9)\n",
        "    del face\n",
        "    gc.collect()\n",
        "\n",
        "    features = fer.model(img_tensor.to(fer.device).float())\n",
        "    #print('6:',torch.cuda.memory_allocated(0)/1e9)\n",
        "    features = features.data.cpu().numpy()\n",
        "    #print('7:',torch.cuda.memory_allocated(0)/1e9)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f142LUpSILDX"
      },
      "outputs": [],
      "source": [
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful'): # truthful\n",
        "  video,audio,metadata = (read_video(os.path.join('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful',filename)))#video in shape T,H,W,C\n",
        "  del audio,metadata\n",
        "  gc.collect()\n",
        "  T,H,W,C = video.shape()\n",
        "  #FERFeatures = torch.empty(T, 1408)\n",
        "  split_size = math.ceil(T/6.0)\n",
        "  listOfTensors = video.split(split_size)\n",
        "  for i in range(len(listOfTensors)):\n",
        "    TensorBatch = torch.squeeze(videoEmbeddingExtraction(torch.unsqueeze(listOfTensors[i])))\n",
        "    del listOfTensors[i]\n",
        "    gc.collect()\n",
        "    if (i == 0):\n",
        "      FERFeatures = TensorBatch\n",
        "      continue\n",
        "    FERFeatures = torch.cat((FERFeatures, TensorBatch))\n",
        "\n",
        "  name = os.path.join('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/TruthfulFERTensors',filename.replace('.mp4','.pt'))\n",
        "  torch.save(FERFeatures, name)\n",
        "  torch.cuda.empty_cache()\n",
        "  del FERFeatures\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6V6XcQ-yeZm",
        "outputId": "3a385691-ffed-4915-de7d-87093c4d030e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "print(device)\n",
        "#model_name='enet_b2_8'\n",
        "\n",
        "#fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "#print(fer)\n",
        "#summary(fer.model,(224,224))\n",
        "#features=fer.extract_features()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkgfkMewGYWA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful'): # truthful\n",
        "\n",
        "video, audio, labels = dataset.__getitem__(10)\n",
        "print(video.shape)\n",
        "print(audio.shape)\n",
        "#frame = video[0]\n",
        "#print(frame.shape)\n",
        "#frame = frame.unsqueeze(0)\n",
        "video = video.unsqueeze(0)\n",
        "audio = audio.unsqueeze(0)\n",
        "print(video.shape)\n",
        "print(audio.shape)\n",
        "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "#video, audio = video.to('cuda'), audio.to('cuda')\n",
        "#print(video.is_cuda)\n",
        "#print(audio.is_cuda)\n",
        "\n",
        "#video = video.cpu().detach().numpy()\n",
        "#audio = audio.cpu().detach().numpy()\n",
        "#outputs = mainModel(video, audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcj0t47Nt_hl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "6e067d45-121e-4c0d-8444-7fc0346ab5ed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-024307b05f64>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mbatchlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/istockphoto-1296158947-612x612.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpermuted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# orignally in C H W for pytorch, after permute in H W C for openCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: '/content/istockphoto-1296158947-612x612.jpg'"
          ]
        }
      ],
      "source": [
        "#will have to delete but keeping for now\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "\n",
        "#Do not forget to run pip install facenet-pytorch\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)\n",
        "\n",
        "def extract_face(frame):\n",
        "    bounding_boxes, probs = mtcnn.detect(frame, landmarks=False)\n",
        "    #bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "    #bounding_box= torch.take_along_dim(torch.from_numpy(bounding_boxes), torch.from_numpy(np.argmax(probs,1)),1)\n",
        "    bounding_boxes2 = []\n",
        "    bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "    for i in range (bounding_boxes.shape[0]):\n",
        "      indexOfBestFact = np.argmax(probs[i])\n",
        "      bounding_boxes2.append(bounding_boxes[i, indexOfBestFact,:])\n",
        "\n",
        "    bounding_boxes2 = np.stack(bounding_boxes2)\n",
        "    bounding_boxes2 = torch.from_numpy(bounding_boxes2).int()\n",
        "    #box = bounding_box.int()\n",
        "    batchlist = []\n",
        "    for batch in range(bounding_boxes2.shape[0]):\n",
        "      x1,y1,x2,y2=bounding_boxes2[batch,0:4]\n",
        "      face_img=frame[batch, y1:y2,x1:x2,:]\n",
        "      batchlist.append(face_img)\n",
        "    return torch.stack(batchlist)\n",
        "img = torchvision.io.read_image('/content/istockphoto-1296158947-612x612.jpg')\n",
        "img = img.unsqueeze(0)\n",
        "permuted = torch.permute(img, (0, 2,3,1))# orignally in C H W for pytorch, after permute in H W C for openCV\n",
        "face = extract_face(permuted)\n",
        "plt.imshow(face[0])\n",
        "print(use_cuda)\n",
        "model_name='enet_b2_8'\n",
        "fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "#features=fer.extract_features(face)\n",
        "#print(features)\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((fer.img_size,fer.img_size)),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                             std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "        )\n",
        "\n",
        "print(face.shape)\n",
        "face = torch.permute(face, (0,3,1,2))\n",
        "print(face.shape)\n",
        "img_tensor = test_transforms(face.double())\n",
        "features = fer.model(img_tensor.to(fer.device).float())\n",
        "print(features.data.max())\n",
        "print(features.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KW_PgiLgxzQ"
      },
      "outputs": [],
      "source": [
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips'):\n",
        "    if filename.endswith('.mp4'):\n",
        "        my_clip = mp.VideoFileClip('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips/'+filename)\n",
        "        my_clip.audio.write_audiofile('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips/'+filename+\".wav\")\n",
        "        #subprocess.call(['ffmpeg', '-i', filename, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '2', filename.replace('.mp4', '.wav')])#changed sampling rate from 44100\n",
        "        #to 16000 becasue that is the sampling rate on which wav2vec2 was trained on and therefor had to downsample audio\n",
        "\n",
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips'):\n",
        "    if filename.endswith('.mp4'):\n",
        "\n",
        "        my_clip = mp.VideoFileClip('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips/'+filename)\n",
        "        my_clip.audio.write_audiofile('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips/'+filename+\".wav\")\n",
        "        #subprocess.call(['ffmpeg', '-i', filename, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '2', filename.replace('.mp4', '.wav')])#changed sampling rate from 44100\n",
        "        #to 16000 becasue that is the sampling rate on which wav2vec2 was trained on and therefor had to downsample audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkenx1LCEtI0"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "\n",
        "\n",
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips'):\n",
        "    if filename.endswith('.wav'):\n",
        "        y, sr = librosa.load('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips/'+filename, sr=16000)\n",
        "        #downsampling data to fit Wav2Vec2\n",
        "        sf.write('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips/'+filename, y, sr, subtype='PCM_24')\n",
        "        #my_clip = mp.VideoFileClip(y)\n",
        "        #my_clip.audio.write_audiofile('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/DeceptiveAudioClips/'+filename)\n",
        "\n",
        "for filename in os.listdir('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips'):\n",
        "    if filename.endswith('.wav'):\n",
        "        y, sr = librosa.load('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips/'+filename, sr=16000)\n",
        "        #downsampling data to fit Wav2Vec2\n",
        "        sf.write('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips/'+filename, y, sr, subtype='PCM_24')\n",
        "        #my_clip = mp.VideoFileClip(y)\n",
        "        #my_clip.audio.write_audiofile('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Audio Clips/TruthfulAudioClips/'+filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE6gIxy-LENj",
        "outputId": "820f4983-19bc-41ef-ef83-de82bbc456f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'face-emotion-recognition' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#start of actualy notebook\n",
        "!git clone https://github.com/HSE-asavchenko/face-emotion-recognition.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZycbN7M_SOco",
        "outputId": "ee13081c-4de3-4e05-eee2-ec554fbc536a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hsemotion\n",
            "  Downloading hsemotion-0.3.0.tar.gz (8.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hsemotion) (1.22.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from hsemotion) (8.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from hsemotion) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from hsemotion) (0.15.2+cu118)\n",
            "Collecting timm (from hsemotion)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->hsemotion) (6.0)\n",
            "Collecting huggingface-hub (from timm->hsemotion)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm->hsemotion)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->hsemotion) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->hsemotion) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->hsemotion) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->hsemotion) (2.27.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->hsemotion) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->hsemotion) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->hsemotion) (23.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->hsemotion) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->hsemotion) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->hsemotion) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->hsemotion) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->hsemotion) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->hsemotion) (1.3.0)\n",
            "Building wheels for collected packages: hsemotion\n",
            "  Building wheel for hsemotion (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsemotion: filename=hsemotion-0.3.0-py3-none-any.whl size=11242 sha256=fc8062a17a85708d83332868df881790cb2428e7b6dad0b4b5680d5e85022c6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/88/e0/3b365122443c2ec55f3e058f2b7ad59df7b5e302c457c4539a\n",
            "Successfully built hsemotion\n",
            "Installing collected packages: safetensors, huggingface-hub, timm, hsemotion\n",
            "Successfully installed hsemotion-0.3.0 huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n",
            "Collecting hsemotion-onnx\n",
            "  Downloading hsemotion-onnx-0.3.1.tar.gz (7.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from hsemotion-onnx) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from hsemotion-onnx) (4.7.0.72)\n",
            "Collecting onnx (from hsemotion-onnx)\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime (from hsemotion-onnx)\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->hsemotion-onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx->hsemotion-onnx) (4.7.1)\n",
            "Collecting coloredlogs (from onnxruntime->hsemotion-onnx)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->hsemotion-onnx) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime->hsemotion-onnx) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->hsemotion-onnx) (1.11.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->hsemotion-onnx)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->hsemotion-onnx) (1.3.0)\n",
            "Building wheels for collected packages: hsemotion-onnx\n",
            "  Building wheel for hsemotion-onnx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hsemotion-onnx: filename=hsemotion_onnx-0.3.1-py3-none-any.whl size=11114 sha256=3af4a84027bbdf60d8721dd4d7f9e3ee1c5fd85e937ec587221df44ea6f0cd0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/e3/e4/159edf450e21bea9099bbd4879cb00d8265f6f612ec03b2425\n",
            "Successfully built hsemotion-onnx\n",
            "Installing collected packages: onnx, humanfriendly, coloredlogs, onnxruntime, hsemotion-onnx\n",
            "Successfully installed coloredlogs-15.0.1 hsemotion-onnx-0.3.1 humanfriendly-10.0 onnx-1.14.0 onnxruntime-1.15.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (0.29.36)\n",
            "Collecting facenet-pytorch\n",
            "  Downloading facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (2.27.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (0.15.2+cu118)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from facenet-pytorch) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->facenet-pytorch) (3.4)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->facenet-pytorch) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision->facenet-pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision->facenet-pytorch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision->facenet-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision->facenet-pytorch) (1.3.0)\n",
            "Installing collected packages: facenet-pytorch\n",
            "Successfully installed facenet-pytorch-2.5.3\n",
            "Collecting timm==0.6.13\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm==0.6.13) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.13) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.6.13) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.6.13) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm==0.6.13) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm==0.6.13) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm==0.6.13) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.6.13) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.6.13) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.6.13) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.6.13) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.13) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.13) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm==0.6.13) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.13) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.6.13) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm==0.6.13) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 0.9.2\n",
            "    Uninstalling timm-0.9.2:\n",
            "      Successfully uninstalled timm-0.9.2\n",
            "Successfully installed timm-0.6.13\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=3d72b3c6c6d1b58f2097bafd32166f1837b5056cddfec81668c863c92c8130a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.27.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.7.1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "Successfully installed sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-10.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install hsemotion\n",
        "!pip install hsemotion-onnx\n",
        "!pip install torch torchvision -U\n",
        "!pip install Cython\n",
        "! pip install facenet-pytorch\n",
        "! pip install timm==0.6.13\n",
        "! pip install ffmpeg\n",
        "! pip install ffmpeg moviepy\n",
        "!pip install librosa\n",
        "!pip install transformers sentencepiece\n",
        "!pip install av"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C26lodVpr7-X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "#from timm.models.layers import conv2d_same\n",
        "#timm.models.layers.conv2d_same\n",
        "from hsemotion.facial_emotions import HSEmotionRecognizer\n",
        "import subprocess # for ffmpeg\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "#Do not forget to run pip install facenet-pytorch\n",
        "from facenet_pytorch import MTCNN\n",
        "from torchvision.io import read_video\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, SpeechEncoderDecoderModel, AutoFeatureExtractor, Wav2Vec2Model\n",
        "#from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "import av\n",
        "\n",
        "#imported specifcally for the Wav2Vec2 Feature Extrators\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "import torchaudio\n",
        "\n",
        "from torchvision.transforms.functional import _interpolation_modes_from_int, InterpolationMode\n",
        "import gc\n",
        "\n",
        "import math\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eurSR34ZtwRM",
        "outputId": "3e85a6ce-4ad0-49f5-9ade-fa85fc52d455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mhVoUyH0PHV",
        "outputId": "e6bfdc0c-5248-448b-e921-6aabbbd9f969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1Q5E68n0z2K"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Deceptive . #deceptive\n",
        "!cp -r /content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips/Truthful . #truthful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q63LYmFyTP1y"
      },
      "outputs": [],
      "source": [
        "# creates dataset used for training\n",
        "class CustomDeceptionDataset(Dataset):\n",
        "    def __init__(self, rootdir):\n",
        "        self.deceptiveClipsPath = os.path.join(rootdir, 'Deceptive')\n",
        "        self.truthfulClipsPath = os.path.join(rootdir, 'Truthful')\n",
        "        self.numTruthful = len(os.listdir(self.truthfulClipsPath))-10 #subtract 10 because I want to preserve 20 of the videos for validation/testing\n",
        "        self.numDeceptive = len(os.listdir(self.deceptiveClipsPath))-10 #subtract 10 because I want to preserve 20 of the videos for validation/testing\n",
        "        self.deceptiveTensorsPath = os.path.join(rootdir,'DeceptiveFERTensors')\n",
        "        self.truthfulTensorsPath = os.path.join(rootdir,'TruthfulFERTensors')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.numTruthful+self.numDeceptive\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if (idx < self.numTruthful):\n",
        "        name = \"trial_truth_%03d.mp4\"%(idx+1)\n",
        "        name2 = os.path.join(self.truthfulTensorsPath, name.replace('mp4','pt'))\n",
        "        name = os.path.join(self.truthfulClipsPath, name)\n",
        "        torchclass = torch.tensor([0]) # 0 for truthful\n",
        "\n",
        "      else:\n",
        "        name = \"trial_lie_%03d.mp4\"%((idx-self.numTruthful)+1)\n",
        "        name2 = os.path.join(self.deceptiveTensorsPath, name.replace('mp4','pt'))\n",
        "        name = os.path.join(self.deceptiveClipsPath, name)\n",
        "        torchclass = torch.tensor([1]) # 1 for deceptive\n",
        "\n",
        "      #my_clip = mp.VideoFileClip(name)\n",
        "      #my_audio = my_clip.audio\n",
        "\n",
        "\n",
        "      unusedvideo,audio,metadata = read_video(name)\n",
        "      fps = metadata['video_fps']\n",
        "      audioSampleRate = metadata['audio_fps']\n",
        "\n",
        "      audio = torchaudio.functional.resample(audio, audioSampleRate, 16000)\n",
        "      audio = torch.mean(audio, dim=0)#takes mean and gets rid of channels dimesnion\n",
        "      video = torch.load(name2, map_location=torch.device('cpu'))\n",
        "\n",
        "      permuted = torch.permute(video, (1,0))#switches embedding and time dimension aka T,F to F,T\n",
        "      normalizedVideoFeatures = torch.nn.functional.avg_pool1d(permuted, math.ceil(fps/10))#because fps varies grossly across the dataset (10,25,29.97,30 fps) I try to average the features so they are close to 10 fps before I padd them to match the audio\n",
        "      averagedVideo = torch.permute(normalizedVideoFeatures, (1,0))\n",
        "      #print(name)\n",
        "      #video = video.unsqueeze(0)#adding batch dimension #Im actually pretty sure your not supposed to add the batch dimension and instead its automatically added by the dataloader\n",
        "      #audio = audio.unsqueeze(0)#adding batch dimension\n",
        "\n",
        "      del normalizedVideoFeatures, permuted, fps, audioSampleRate, unusedvideo\n",
        "      gc.collect()\n",
        "\n",
        "      return averagedVideo,audio,torchclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom dataset containing 10 lying and 10 truthful videos used for training\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, rootdir):\n",
        "        self.deceptiveClipsPath = os.path.join(rootdir, 'Deceptive')\n",
        "        self.truthfulClipsPath = os.path.join(rootdir, 'Truthful')\n",
        "        self.numTruthful = 10\n",
        "        self.numDeceptive = 10\n",
        "        self.deceptiveTensorsPath = os.path.join(rootdir,'DeceptiveFERTensors')\n",
        "        self.truthfulTensorsPath = os.path.join(rootdir,'TruthfulFERTensors')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.numTruthful+self.numDeceptive\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      if (idx < self.numTruthful):\n",
        "        name = \"trial_truth_%03d.mp4\"%(idx+51)#have to add 1 because 0 indexed and have to add 50 so it starts on right vid\n",
        "        name2 = os.path.join(self.truthfulTensorsPath, name.replace('mp4','pt'))\n",
        "        name = os.path.join(self.truthfulClipsPath, name)\n",
        "        torchclass = torch.tensor([0])\n",
        "\n",
        "      else:\n",
        "        name = \"trial_lie_%03d.mp4\"%((idx-self.numTruthful)+52)#have to add 1 because 0 indexed and have to add 51 so it starts on right vid\n",
        "        name2 = os.path.join(self.deceptiveTensorsPath, name.replace('mp4','pt'))\n",
        "        name = os.path.join(self.deceptiveClipsPath, name)\n",
        "        torchclass = torch.tensor([1])\n",
        "\n",
        "      #my_clip = mp.VideoFileClip(name)\n",
        "      #my_audio = my_clip.audio\n",
        "\n",
        "\n",
        "      unusedvideo,audio,metadata = read_video(name)\n",
        "      fps = metadata['video_fps']\n",
        "      audioSampleRate = metadata['audio_fps']\n",
        "\n",
        "      audio = torchaudio.functional.resample(audio, audioSampleRate, 16000)\n",
        "      audio = torch.mean(audio, dim=0)#takes mean and gets rid of channels dimesnion\n",
        "      video = torch.load(name2, map_location=torch.device('cpu'))\n",
        "\n",
        "      permuted = torch.permute(video, (1,0))#switches embedding and time dimension aka T,F to F,T\n",
        "      normalizedVideoFeatures = torch.nn.functional.avg_pool1d(permuted, math.ceil(fps/10))#because fps varies grossly across the dataset (10,25,29.97,30 fps) I try to average the features so they are close to 10 fps before I padd them to match the audio\n",
        "      averagedVideo = torch.permute(normalizedVideoFeatures, (1,0))\n",
        "      #print(name)\n",
        "      #video = video.unsqueeze(0)#adding batch dimension #Im actually pretty sure your not supposed to add the batch dimension and instead its automatically added by the dataloader\n",
        "      #audio = audio.unsqueeze(0)#adding batch dimension\n",
        "\n",
        "      del normalizedVideoFeatures, permuted, fps, audioSampleRate, unusedvideo\n",
        "      gc.collect()\n",
        "\n",
        "      return averagedVideo,audio,torchclass"
      ],
      "metadata": {
        "id": "AYdyGXIGRlw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEakSihp-iLa"
      },
      "outputs": [],
      "source": [
        "class MultimodalBottleneckFusion (nn.Module):\n",
        "  def __init__(self):\n",
        "   super(MultimodalBottleneckFusion, self).__init__()\n",
        "  #Standard transformer encoders use 6 layers (and 6 decoder layers) but the MBT paper uses 12. Should probably use 6 because you already pre-process audio and video\n",
        "   self.audio_layers = [nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device),\n",
        "                        nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device),\n",
        "                        nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device)]\n",
        "\n",
        "   self.video_layers = [nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device),\n",
        "                        nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device),\n",
        "                        nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device), nn.TransformerEncoderLayer(d_model=544, nhead=8).to(device)]\n",
        "\n",
        "   self.video_layer_0 = nn.TransformerEncoderLayer(d_model=512, nhead=8).to(device) #send rgb FER vectors here before for-loop in order to calculate pairwise self attention\n",
        "\n",
        "  def __call__(self,video_in, audio_in):\n",
        "    bottleneck = torch.empty(1,video_in.shape[1],32).normal_(mean=0,std=0.02).to(device) # creates random Gaussian distribution like they did in paper\n",
        "    #orignally had 50 in above code for dim1 (second dimension)\n",
        "    curr_video = self.video_layer_0(video_in)\n",
        "    curr_audio = audio_in\n",
        "    for i in range(6):\n",
        "\n",
        "      video_layer_input = torch.cat((curr_video, bottleneck), 2)\n",
        "      audio_layer_input = torch.cat((curr_audio, bottleneck), 2)\n",
        "\n",
        "      video_layer_output = self.video_layers[i](video_layer_input.to(device))\n",
        "      audio_layer_output = self.audio_layers[i](audio_layer_input.to(device))\n",
        "\n",
        "      curr_video = video_layer_output[:, :, 0:512]\n",
        "      bottleneck_video_input = video_layer_output[:, :, 512:544]\n",
        "\n",
        "      curr_audio = audio_layer_output[:, :, 0:512]\n",
        "      bottleneck_audio_input = audio_layer_output[:, :, 512:544]\n",
        "\n",
        "      bottleneck = torch.mean(torch.stack((bottleneck_audio_input, bottleneck_video_input)), dim=0)\n",
        "\n",
        "    del bottleneck\n",
        "    gc.collect()\n",
        "\n",
        "    return curr_video, curr_audio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJYf936vRHzP"
      },
      "outputs": [],
      "source": [
        "class Classifier (nn.Module):\n",
        "  #***putting n for input of first linear layer because I don't know the exact size of it yet\n",
        "  def __init__(self):\n",
        "   super(Classifier, self).__init__()\n",
        "   #self.first_Linear_Layer = nn.Linear(how do i specify the dimension i want to operate on?)#combines the audio and video features of each time-step\n",
        "   self.first_Linear_Layer = nn.Linear(1024, 1)#takes output of first Linear Layer and compresses each of the temporal 1d vectors to size 1\n",
        "   #RELU #applies activation function to outputs of second Linear Layer. gets rid of noisy ot uneventful time steps that are negative (assuming 0 is truth)\n",
        "   ##not sure if i should RELU, or ELU, or Leaky ELU. Which one should I use?\n",
        "   #self.final_Linear_Layer = nn.Linear()#instead of linear layer do max average or average\n",
        "   #takes output from RELU activation function and gives final 1,1 logit which is passed into Binary Cross Entropy with built in softmax\n",
        "   #self.audio_Linear_Layer = nn.Linear(512,1)\n",
        "   #self.video_Linear_Layer = nn.Linear(512,1)\n",
        "   #self.combineModes = nn.Linear(2,1) # exactly as the same as just oding 1024,1 on the first linear layer\n",
        "\n",
        "  def __call__(self,video_input,audio_input):\n",
        "    bimodal = torch.cat((video_input,audio_input),2)#try hadamard product later 1,50,1024\n",
        "    condensed = self.first_Linear_Layer(bimodal)\n",
        "    average = torch.mean(condensed,dim=1)\n",
        "\n",
        "    del bimodal, condensed\n",
        "    gc.collect()\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #audio_output = self.audio_Linear_Layer(audio_input)\n",
        "    #video_input = self.video_Linear_Layer(video_input)\n",
        "    #bimodal = torch.cat((video_input,audio_output),2)\n",
        "    #combinedmodes = self.combineModes(bimodal)\n",
        "    #probability = torch.mean(combinedmodes,dim=1)\n",
        "    #signoidProbability = torch.signoid()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjOW40xTKMUJ"
      },
      "outputs": [],
      "source": [
        "class DeceptionModel (nn.Module):\n",
        "  def __init__(self):\n",
        "   super(DeceptionModel, self).__init__()\n",
        "   self.classifer = Classifier()\n",
        "   self.transformerEncoder = MultimodalBottleneckFusion()\n",
        "   self.bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
        "   self.AudioVectorizer = self.bundle.get_model()\n",
        "   #self.AudioVectorizer = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base-960h')#facebook/wav2vec2-base-960h facebook/wav2vec2-base\n",
        "   #self.AudioVectorizer = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
        "   self.AudioVectorizer.requires_grad = True\n",
        "   self.videoCompression = nn.Linear(1408,512)\n",
        "   self.audioCompression = nn.Linear(768,512)\n",
        "\n",
        "  def __call__ (self, FERvideo, audio): #shape of audio is B C(channels) T(16000 * duration in seconds) shape of my video is B T H W C (actually since this is the video feautres shape is B T F)\n",
        "    #print(FERvideo.shape)\n",
        "    #print(audio.shape)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = 'cuda' if use_cuda else 'cpu'\n",
        "    #print(device)\n",
        "    #FERvideo, audio = FERvideo.to(device), audio.to(device)#tensors have to be on GPU because Wav2Vec2 model weights are on gpu\n",
        "\n",
        "\n",
        "\n",
        "    #print(torch.argmax(audio,dim))\n",
        "    #vectorizedAudio = self.AudioVectorizer(audio, False, None, False, None, None, 'pt', 16000)#.last_hidden_state\n",
        "    #vectorizedAudio = self.AudioVectorizer(audio).last_hidden_state\n",
        "    vectorizedAudioList = self.AudioVectorizer.extract_features(audio)\n",
        "    del audio\n",
        "    gc.collect()\n",
        "\n",
        "    ## vectorizedAudioList is a list of list of tensors, so the below code is turning it into a tensor. Actually dim of first list is only 1 so its just list of tensors\n",
        "    # each tensor in list is formatted like so: `(batch, time frame, feature dimension)`\n",
        "    # the batch dimension was 1 because I only passed in 1 input, and it will continue like that until I switch batch size away from 1\n",
        "    # however, there are 12 different elements of the array. they are all the same size and the batch dimension was already included in each individual tensor\n",
        "    # so I think it is just 12 different features the thing picked up on. It could be that it cut the sound into 12 time frames, but I doubt it since the time dim\n",
        "    # seems to roughly match the 50 sample rate * duration of vid in seconds\n",
        "\n",
        "    vectorizedAudioTensor = vectorizedAudioList[0][0]\n",
        "    vectorizedAudioTensor = torch.unsqueeze(vectorizedAudioTensor, 0)\n",
        "\n",
        "    for i in range (len(vectorizedAudioList[0])-1):\n",
        "\n",
        "      vectorizedAudioTensor = torch.cat((vectorizedAudioTensor, torch.unsqueeze(vectorizedAudioList[0][i+1],0)), 0)\n",
        "\n",
        "    del vectorizedAudioList\n",
        "    gc.collect()\n",
        "    # we added all the elements in the array across a new dimension by unsqueezing to preserve the batch size. It should now be 12, B, T, F\n",
        "\n",
        "    #for i in range (len(vectorizedAudioList)-1):\n",
        "    #  print(i)\n",
        "    #  tempTensor = torch.unsqueeze(vectorizedAudioList[i+1][0], 0)\n",
        "    #  tempTensor = torch.unsqueeze(tempTensor, 0)\n",
        "    #  for j in range (len(vectorizedAudioList[i+1])-1):\n",
        "    #    tempTensor = torch.cat((tempTensor, torch.unsqueeze(torch.unsqueeze(vectorizedAudioList[i+1][j+1],0),0)), 1)\n",
        "    #  vectorizedAudioTensor = torch.cat((vectorizedAudioTensor, tempTensor),0)\n",
        "\n",
        "    vectorizedAudioTensor = torch.mean(vectorizedAudioTensor, dim=0) # averages 12 transformer encoder heads outputted by wav2vec\n",
        "\n",
        "    #vectorizedAudio = torch.Tensor(vectorizedAudio)\n",
        "\n",
        "    paddedAudio = torch.cat((vectorizedAudioTensor, torch.zeros((vectorizedAudioTensor.shape[0], 1, vectorizedAudioTensor.shape[2])).to(device)), 1) #fixes audios so 50 samples per second instead of 50*seconds -1\n",
        "    permuted = torch.permute(paddedAudio, (0,2,1))#switches embedding ant time dimension aka switches B T F to B F T\n",
        "    condensedAudioVecs = torch.nn.functional.avg_pool1d(permuted, 5)\n",
        "    averagedAudio = torch.permute(condensedAudioVecs, (0,2,1))#now this averaged audio is BTF (f = 768) where T is 10 FPS\n",
        "    transformerInputAudio = self.audioCompression(averagedAudio)\n",
        "\n",
        "    #pass it through FER feature extractors\n",
        "    #FERfeatures = self.videoEmbeddingExtraction(video)\n",
        "    #print(FERfeatures.shape)\n",
        "    #breakpoint\n",
        "    transformerInputVideo = self.videoCompression(FERvideo)\n",
        "\n",
        "    # padding video and audio to be same size\n",
        "    B,TimeVideo,F = transformerInputVideo.shape\n",
        "    B,TimeAudio,F = transformerInputAudio.shape\n",
        "\n",
        "    if (TimeVideo > TimeAudio):\n",
        "      transformerInputAudio = torch.cat((transformerInputAudio, torch.zeros((transformerInputAudio.shape[0], TimeVideo-TimeAudio, transformerInputAudio.shape[2])).to(device)), 1)\n",
        "\n",
        "    elif (TimeAudio > TimeVideo):\n",
        "      transformerInputVideo = torch.cat((transformerInputVideo, torch.zeros((transformerInputVideo.shape[0], TimeAudio-TimeVideo, transformerInputVideo.shape[2])).to(device)), 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    videoEmbed, audioEmbed = self.transformerEncoder(transformerInputVideo.to(device), transformerInputAudio.to(device))\n",
        "    FinalLogitTruthFalse = self.classifer(videoEmbed, audioEmbed) # 0 for truth 1 for lie\n",
        "\n",
        "    del paddedAudio,permuted, condensedAudioVecs, averagedAudio, transformerInputAudio, transformerInputVideo, videoEmbed, audioEmbed, vectorizedAudioTensor, B, TimeVideo, TimeAudio, F\n",
        "    gc.collect()\n",
        "    return FinalLogitTruthFalse\n",
        "\n",
        "#  def videoEmbeddingExtraction (self, video): # manually implementing some of the stuff from HSME\n",
        "#    #video dimensions are B T H W C\n",
        "#    # dimensions for each frame (which extract face requires) B'(B*T) H W C\n",
        "#    B,T,H,W,C = video.shape\n",
        "#    #cv2_imshow(torch.squeeze(torch.squeeze(video[0,0,:,:,:])).cpu().numpy())\n",
        "#    mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)\n",
        "#    def extract_face(frame):\n",
        "#        bounding_boxes, probs = mtcnn.detect(frame, landmarks=False)\n",
        "#        print(bounding_boxes.shape)\n",
        "#        #bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "#        #bounding_box= torch.take_along_dim(torch.from_numpy(bounding_boxes), torch.from_numpy(np.argmax(probs,1)),1)\n",
        "#        bounding_boxes2 = []\n",
        "#        #bounding_boxes = np.vstack(bounding_boxes[:]).astype(np.float64)\n",
        "#        #print(bounding_boxes.shape)\n",
        "#        #bounding_boxes = bounding_boxes.astype(np.float64)\n",
        "#        for i in range (bounding_boxes.shape[0]):\n",
        "#          indexOfBestFace = np.argmax(probs[i])\n",
        "#          arrayOfBoundingBoxesForOneFrame = bounding_boxes[i]\n",
        "#          bounding_boxes2.append(arrayOfBoundingBoxesForOneFrame[indexOfBestFace])#originalbounding_boxes2.append(bounding_boxes[i, indexOfBestFact,:])\n",
        "#\n",
        "#        del indexOfBestFace\n",
        "#        gc.collect()\n",
        "#\n",
        "#        del arrayOfBoundingBoxesForOneFrame\n",
        "#        gc.collect()\n",
        "\n",
        "#        del bounding_boxes\n",
        "#        gc.collect()\n",
        "#\n",
        "#        bounding_boxes2 = np.stack(bounding_boxes2)\n",
        "#        bounding_boxes2 = torch.from_numpy(bounding_boxes2.astype('int32'))#.int()\n",
        "#        #box = bounding_box.int()\n",
        "#        batchlist = []\n",
        "#        i = 0\n",
        "#        height = 0\n",
        "#        width = 0\n",
        "#        for batch in range(bounding_boxes2.shape[0]):\n",
        "#          i += 1\n",
        "#          x1,y1,x2,y2=bounding_boxes2[batch,0:4]\n",
        "#          face_img=frame[batch, y1:y2,x1:x2,:] # batch, h, w, c\n",
        "#          batchlist.append(face_img)\n",
        "#          height += face_img.shape[0]\n",
        "#          width += face_img.shape[1]\n",
        "\n",
        "#        averageHeigth = int(height/i)\n",
        "#        averageWidth = int(width/i)\n",
        "\n",
        "#        del bounding_boxes2\n",
        "#        gc.collect()\n",
        "#\n",
        "#        print(averageHeigth)\n",
        "#        print(averageWidth)\n",
        "#        resize = transforms.Compose([transforms.Resize((averageHeigth,averageWidth),interpolation=InterpolationMode.BILINEAR, max_size=None, antialias=True)])\n",
        "\n",
        "#        for j in range(i):#i should be equal to bounding_boxes2.shape[0]\n",
        "#          batchlist[j] = resize(torch.permute(batchlist[j], (2,0,1)))#swithcing from hwc to chw and then resizing image\n",
        "\n",
        "#        return torch.stack(batchlist)\n",
        "\n",
        "#    facialRecognitionBatch  = video.reshape(B*T, H, W, C) #got rid of time dimension from input\n",
        "#    del video\n",
        "#    gc.collect()\n",
        "    #print(facialRecognitionBatch.shape)\n",
        "    #permuted = torch.permute(facialRecognitionBatch, (0, 2,3,1))# orignally in C H W for pytorch, after permute in H W C for openCV\n",
        "    #cv2_imshow(torch.squeeze(facialRecognitionBatch[0,:,:,:]).cpu().numpy())\n",
        "\n",
        "    #facialRecognitionBatchNumpy = facialRecognitionBatch.cpu().numpy()\n",
        "    #facialRecognitionBatchNumpyBGR = facialRecognitionBatchNumpy[:,:,:,::-1]\n",
        "    #tensorBatch = torch.from_numpy(facialRecognitionBatchNumpyBGR)\n",
        "    #red, green, blue = torch.split(facialRecognitionBatch,3,3)#converting to BGR for open cv\n",
        "    #facialRecognitionBatch_BGR = torch.cat(3, [blue,green,red])\n",
        "    #cv2_imshow(torch.squeeze(facialRecognitionBatch_BGR[0,:,:,:]).cpu().numpy())\n",
        "#    face = extract_face(facialRecognitionBatch) # B C H W\n",
        "\n",
        "#    del facialRecognitionBatch\n",
        "#    gc.collect()\n",
        "\n",
        "#    torch.cuda.empty_cache()\n",
        "\n",
        "#    print(\"face.shape\")\n",
        "#    print(face.shape)\n",
        "#    model_name='enet_b2_8'\n",
        "#    fer=HSEmotionRecognizer(model_name=model_name,device=device)\n",
        "    #features=fer.extract_features(face)\n",
        "    #print(features)\n",
        "\n",
        "#    test_transforms = transforms.Compose(\n",
        "#                [z\n",
        "#                    transforms.Resize((fer.img_size,fer.img_size)),\n",
        "#                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                                std=[0.229, 0.224, 0.225])\n",
        "#                ]\n",
        "#            )\n",
        "\n",
        "#    #face = torch.permute(face, (0,3,1,2))\n",
        "#    img_tensor = test_transforms(face.double())\n",
        "#\n",
        "#    del face\n",
        "#    gc.collect()\n",
        "\n",
        "#    features = fer.model(img_tensor.to(fer.device).float())\n",
        "#    features = features.reshape(B,T,-1)\n",
        "#    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCuoi20Z__PK"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDeceptionDataset('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips')\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "training_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)#put batchsize to 1 for now only because videos are diff lenghts\n",
        "#after make videos the same lenght will swithc batch size back to 4 and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTYzwC7KJyWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c22cc4f-f8a1-4a3d-bd47-a25f9b1f0030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960.pth\n",
            "100%|██████████| 360M/360M [00:03<00:00, 95.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = 'cuda' if use_cuda else 'cpu'\n",
        "mainModel = DeceptionModel()\n",
        "mainModel.to(device)#what is cuda? should i use that?\n",
        "lossFunction = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(mainModel.parameters(), lr=0.001)#orinally had momentum=0.9 as a paramater but didnt work so scraped it\n",
        "##instantiate vide model\n",
        "##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ_XmDzABrLh"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        video, audio, labels = data\n",
        "        video, audio, labels = video.to(device), audio.to(device), labels.to(device)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "        #video = video[:,0:300,:]\n",
        "        #audio = audio[:,0:500000]\n",
        "        # Make predictions for this batch\n",
        "        outputs = mainModel(video, audio)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "\n",
        "        loss = lossFunction(outputs, labels.float())\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "        del video, audio, labels, outputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        #print(torch.cuda.memory_summary())\n",
        "\n",
        "    return last_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWlnPVmZDDeu"
      },
      "outputs": [],
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "    use_cuda = torch.cuda.is_available() # this and the two lines after are just meant to make sure its utilizer gpu\n",
        "    device = 'cuda' if use_cuda else 'cpu'\n",
        "    mainModel.to(device)\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    mainModel.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    #running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    #model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    #with torch.no_grad():\n",
        "    #    for i, vdata in enumerate(validation_loader):\n",
        "    #        vinputs, vlabels = vdata\n",
        "    #        voutputs = model(vinputs)\n",
        "    #        vloss = loss_fn(voutputs, vlabels)\n",
        "    #        running_vloss += vloss\n",
        "    #\n",
        "    #avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {}'.format(avg_loss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training',\n",
        "                    { 'Training' : avg_loss},\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    #if avg_vloss < best_vloss:\n",
        "    #    best_vloss = avg_vloss\n",
        "    model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "    torch.save(mainModel.state_dict(), '/content/drive/MyDrive/ColabNotebooks/'+model_path)# refer to 2:46 Patrick Loeber \"Saving and Loading Models\" video\n",
        "\n",
        "    epoch_number += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testingSet = TestDataset('/content/drive/MyDrive/RealLifeDeceptionDetection.2016/Real-life_Deception_Detection_2016/Clips')\n",
        "# Create data loaders for our datasets; shuffle for training, not for validation\n",
        "testing_loader = torch.utils.data.DataLoader(testingSet, batch_size=1, shuffle=False)#put batchsize to 1 for now only because videos are diff lenghts\n",
        "#after make videos the same lenght will swithc batch size back to 4 and"
      ],
      "metadata": {
        "id": "brxUXIC7USRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainModel.load_state_dict(torch.load(\"/content/drive/MyDrive/ColabNotebooks/model_20230718_234754_4\"))\n",
        "mainModel.eval()\n",
        "\n",
        "\n",
        "running_loss = 0.0\n",
        "correctanswers = 0\n",
        "truepositive = 0\n",
        "truenegative = 0\n",
        "falsenegative = 0\n",
        "falsepositive = 0\n",
        "\n",
        "for i, data in enumerate(testing_loader):\n",
        "        with torch.no_grad():\n",
        "          # Every data instance is an input + label pair\n",
        "          video, audio, labels = data\n",
        "          video, audio, labels = video.to(device), audio.to(device), labels.to(device)\n",
        "\n",
        "          # Zero your gradients for every batch!\n",
        "\n",
        "          # Make predictions for this batch\n",
        "          outputs = mainModel(video, audio)\n",
        "          print(outputs)\n",
        "\n",
        "          print(labels.item())\n",
        "\n",
        "          if (outputs.item() >= 0):\n",
        "              answer = 1\n",
        "          else:\n",
        "              answer = 0\n",
        "          if (answer == labels.item()):\n",
        "              correctanswers += 1\n",
        "              if (answer == 1):\n",
        "                truepositive += 1\n",
        "              else:\n",
        "                truenegative += 1\n",
        "          else:\n",
        "              if (answer == 1):\n",
        "                falsepositive += 1\n",
        "              else:\n",
        "                falsenegative += 1\n",
        "\n",
        "\n",
        "\n",
        "          # Compute the loss and its gradients\n",
        "\n",
        "          loss = lossFunction(outputs, labels.float())\n",
        "\n",
        "\n",
        "          # Gather data and report\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          del video, audio, labels, outputs\n",
        "          gc.collect()\n",
        "\n",
        "print(running_loss/20)\n",
        "\n",
        "print('Correct answers: {}'.format(correctanswers))\n",
        "print('True positive: {}'.format(truepositive))\n",
        "print('True negative: {}'.format(truenegative))\n",
        "print('False positive: {}'.format(falsepositive))\n",
        "print('False negative: {}'.format(falsenegative))\n",
        "print('Accuracy: {}'.format(correctanswers/20))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JCwc1PO25w_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac26c15c-c842-419e-954f-6b715bbd1e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
            "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.3851]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3601]], device='cuda:0')\n",
            "0\n",
            "tensor([[0.9665]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3604]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3034]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.0820]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3583]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3849]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3599]], device='cuda:0')\n",
            "0\n",
            "tensor([[0.9669]], device='cuda:0')\n",
            "0\n",
            "tensor([[1.3416]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.2905]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3284]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3362]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3803]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3526]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3493]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3707]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3484]], device='cuda:0')\n",
            "1\n",
            "tensor([[1.3206]], device='cuda:0')\n",
            "1\n",
            "0.869377176463604\n",
            "Correct answers: 10\n",
            "True positive: 10\n",
            "True negative: 0\n",
            "False positive: 10\n",
            "False negative: 0\n",
            "Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}